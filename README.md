# Temperature Awareness Experiment

This project investigates whether large language models (LLMs) can accurately report the sampling temperature parameter being used to generate their responses.

## Overview

The experiment tests multiple models across different temperature settings (0.0, 0.5, 1.0) and measures how accurately they can self-report their temperature. We also explore whether providing "warmup" questions before asking about temperature affects accuracy.

### Models Tested

- OpenAI GPT-5.2
- Claude Haiku 4.5
- Claude Sonnet 4.5
- Claude Opus 4.5

### Experiment Design

1. **Direct prompts (N=0)**: Ask the model directly to report its temperature with 20 different prompt variations
2. **Warmup prompts (N=1,2,4,8,16)**: Provide N general knowledge questions from Alpaca before asking about temperature

## Installation

```bash
# Install dependencies with uv
uv sync
```

## Environment Setup

Create a `.env` file with your API keys:

```
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
```

## Usage

### Run the Experiment

```bash
uv run python src/run_experiment.py
```

This runs all experiments and saves results to `results/experiment_results.json`.

### Analyze Results

```bash
# Generate per-temperature plots (reported temp vs warmup questions)
uv run python src/analyze_results.py

# Generate L1/L2 error plots (combined across all temperatures)
uv run python src/plot_errors.py
```

## Output Plots

### Per-Temperature Plots

Generated by `analyze_results.py`:

- `results/temp_0.0_vs_n_questions.png` - Reported temperature vs warmup questions at T=0.0
- `results/temp_0.5_vs_n_questions.png` - Reported temperature vs warmup questions at T=0.5
- `results/temp_1.0_vs_n_questions.png` - Reported temperature vs warmup questions at T=1.0

These plots show how the average reported temperature varies with the number of warmup questions, for each model at a specific actual temperature.

### Error Plots

Generated by `plot_errors.py`:

- `results/l1_error_vs_n_questions.png` - Mean Absolute Error (L1) vs warmup questions
- `results/l2_error_vs_n_questions.png` - Mean Squared Error (L2) vs warmup questions

These plots aggregate across all temperature settings to show overall prediction error:
- **L1 Error**: `|reported_temperature - actual_temperature|`
- **L2 Error**: `(reported_temperature - actual_temperature)²`

## Project Structure

```
temperature-awareness/
├── src/
│   ├── run_experiment.py    # Main experiment runner
│   ├── analyze_results.py   # Generate per-temperature plots
│   ├── plot_errors.py       # Generate L1/L2 error plots
│   ├── prompts.py           # Prompt templates
│   ├── alpaca_questions.py  # Warmup questions from Alpaca
│   └── autograder.py        # Parse temperature from responses
├── results/
│   ├── experiment_results.json
│   └── *.png                # Generated plots
├── pyproject.toml
└── README.md
```
